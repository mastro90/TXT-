{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bianc\\anaconda3\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: torch in c:\\users\\bianc\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\bianc\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from torchvision) (1.21.5)\n",
      "Requirement already satisfied: requests in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from torchvision) (2.27.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bianc\\anaconda3\\lib\\site-packages (from requests->torchvision) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"nvidia-smi\" non ï¿½ riconosciuto come comando interno o esterno,\n",
      " un programma eseguibile o un file batch.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as LV\n",
    "\n",
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "#import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.13.1+cpu Device: cpu \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bianc\\AppData\\Local\\Temp\\ipykernel_6020\\1883626309.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  assert(LV(torch.__version__) >= LV(\"1.0.0\"))\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    devicename = '['+torch.cuda.get_device_name(0)+']'\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    devicename = \"\"\n",
    "    \n",
    "print('Using PyTorch version:', torch.__version__,\n",
    "      'Device:', device, devicename)\n",
    "assert(LV(torch.__version__) >= LV(\"1.0.0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BERTdataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toxicity</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_clean</th>\n",
       "      <th>tweet_tokenized</th>\n",
       "      <th>tweet_tok=2</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so...</td>\n",
       "      <td>father dysfunctional selfish drags kids dysfun...</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drags'...</td>\n",
       "      <td>[('father', 'dysfunctional'), ('dysfunctional'...</td>\n",
       "      <td>['father', 'dysfunctional', 'selfish', 'drag',...</td>\n",
       "      <td>father dysfunctional selfish drag kid dysfunct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks lyft credit  can not use cause  do not ...</td>\n",
       "      <td>['thanks', 'lyft', 'credit', 'can', 'not', 'us...</td>\n",
       "      <td>[('thanks', 'lyft'), ('lyft', 'credit'), ('cre...</td>\n",
       "      <td>['thank', 'lyft', 'credit', 'can', 'not', 'use...</td>\n",
       "      <td>thank lyft credit can not use cause do not off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday majesty</td>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>[('bihday', 'majesty')]</td>\n",
       "      <td>['bihday', 'majesty']</td>\n",
       "      <td>bihday majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>[('factsguide', 'society'), ('society', 'motiv...</td>\n",
       "      <td>['factsguide', 'society', 'motivation']</td>\n",
       "      <td>factsguide society motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[2/2] huge fan fare and big talking before the...</td>\n",
       "      <td>huge fan fare big talking leave chaos pay disp...</td>\n",
       "      <td>['huge', 'fan', 'fare', 'big', 'talking', 'lea...</td>\n",
       "      <td>[('huge', 'fan'), ('fan', 'fare'), ('fare', 'b...</td>\n",
       "      <td>['huge', 'fan', 'fare', 'big', 'talk', 'leave'...</td>\n",
       "      <td>huge fan fare big talk leave chaos pay dispute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Toxicity                                              tweet  \\\n",
       "0         0  @user when a father is dysfunctional and is so...   \n",
       "1         0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         0                                bihday your majesty   \n",
       "3         0             factsguide: society now    #motivation   \n",
       "4         0  [2/2] huge fan fare and big talking before the...   \n",
       "\n",
       "                                         tweet_clean  \\\n",
       "0  father dysfunctional selfish drags kids dysfun...   \n",
       "1  thanks lyft credit  can not use cause  do not ...   \n",
       "2                                     bihday majesty   \n",
       "3                      factsguide society motivation   \n",
       "4  huge fan fare big talking leave chaos pay disp...   \n",
       "\n",
       "                                     tweet_tokenized  \\\n",
       "0  ['father', 'dysfunctional', 'selfish', 'drags'...   \n",
       "1  ['thanks', 'lyft', 'credit', 'can', 'not', 'us...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3            ['factsguide', 'society', 'motivation']   \n",
       "4  ['huge', 'fan', 'fare', 'big', 'talking', 'lea...   \n",
       "\n",
       "                                         tweet_tok=2  \\\n",
       "0  [('father', 'dysfunctional'), ('dysfunctional'...   \n",
       "1  [('thanks', 'lyft'), ('lyft', 'credit'), ('cre...   \n",
       "2                            [('bihday', 'majesty')]   \n",
       "3  [('factsguide', 'society'), ('society', 'motiv...   \n",
       "4  [('huge', 'fan'), ('fan', 'fare'), ('fare', 'b...   \n",
       "\n",
       "                                    tweet_lemmatized  \\\n",
       "0  ['father', 'dysfunctional', 'selfish', 'drag',...   \n",
       "1  ['thank', 'lyft', 'credit', 'can', 'not', 'use...   \n",
       "2                              ['bihday', 'majesty']   \n",
       "3            ['factsguide', 'society', 'motivation']   \n",
       "4  ['huge', 'fan', 'fare', 'big', 'talk', 'leave'...   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  father dysfunctional selfish drag kid dysfunct...  \n",
       "1  thank lyft credit can not use cause do not off...  \n",
       "2                                    bihday majesty   \n",
       "3                     factsguide society motivation   \n",
       "4  huge fan fare big talk leave chaos pay dispute...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding target variable\n",
    "encoder = LabelEncoder()\n",
    "X = df['tweet_clean']\n",
    "#y = df['Toxicity']\n",
    "y = encoder.fit_transform(df[\"Toxicity\"]) \n",
    "# Split in train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1000, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target in tensors\n",
    "train_labels = torch.tensor(y_train)\n",
    "test_labels = torch.tensor(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXT PROCESSING PER BERT\n",
    "# (1) Tokenize the sentence.\n",
    "###  (3) Prepend the `[CLS]` token to the start. Append the `[SEP]` token to the end.\n",
    "###   (2) Map tokens to their IDs.\n",
    "###   (4) Pad or truncate the sentence to `max_length`\n",
    "###   (5) Create attention masks for [PAD] tokens.\n",
    "Function tokenizer.encode_plus was used which encapsulates the whole procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1)\n",
    "# Load the BERT tokenizer.\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized tweet:  ['hell', 'wrong', 'humanity', 'would', 'anyone', 'need', 'anything', 'prove']\n",
      "Original tweet:  hell wrong humanity would anyone need anything prove\n"
     ]
    }
   ],
   "source": [
    "# Example for a tweet \n",
    "print(\"Tokenized tweet: \", tokenizer.tokenize(X_train.values[0]))\n",
    "print(\"Original tweet: \", X_train.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:  [3109, 3308, 8438, 2052, 3087, 2342, 2505, 6011]\n"
     ]
    }
   ],
   "source": [
    "# (2)\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(X_train.values[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized tweet:  ['[CLS]', 'hell', 'wrong', 'humanity', 'would', 'anyone', 'need', 'anything', 'prove', '[SEP]']\n",
      "Token IDs:  [101, 3109, 3308, 8438, 2052, 3087, 2342, 2505, 6011, 102]\n"
     ]
    }
   ],
   "source": [
    "# (3)\n",
    "# Example complete:\n",
    "#To be properly handled by BERT we need to add the [CLS] token at the beginning of text. Il token [SEP] Ã¨ un altro token speciale richiesto da BERT alla fine della frase.\n",
    "sentence = X_train.values[0]\n",
    "tokens_ = tokenizer.tokenize(sentence)\n",
    "tokens_ = ['[CLS]'] + tokens_ + ['[SEP]']\n",
    "print(\"Tokenized tweet: \", tokens_)\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokens_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RICERCA MAX LENGTH TRAIN & TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train = [len(i.split()) for i in X_train]\\ntest = [len(i.split()) for i in X_test]\\n\\nfor i in [train, test]:\\n  pd.Series(i).hist(bins=20)'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(4)\n",
    "#Ora impostiamo le lunghezze massime della sequenza per le nostre frasi di addestramento e test come MAX_LEN_TRAIN e MAX_LEN_TEST. \n",
    "#La lunghezza massima supportata dal modello BERT utilizzato Ã¨ 128 e 512.\n",
    "#Ma noi ricerchiamo la massima adatta al task --> prendo le frasi e le divido per trovare di quante parole sono composte --> Ã¨ la lunghezza della frase \n",
    "#se lo facciamo per ogni frase poi vediamo quale Ã¨ la piÃ¹ lunga e prendiamo come valore massimo quello\n",
    "\"\"\"train = [len(i.split()) for i in X_train]\n",
    "test = [len(i.split()) for i in X_test]\n",
    "\n",
    "for i in [train, test]:\n",
    "  pd.Series(i).hist(bins=20)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bianc\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 128\n",
    "#encode train set\n",
    "for i in X_train:\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(i,\n",
    "                                                    add_special_tokens = True,\n",
    "                                                    return_attention_mask = True,\n",
    "                                                    pad_to_max_length = True,\n",
    "                                                    max_length = MAX_LENGTH,\n",
    "                                                    return_tensors = 'pt')\n",
    "                                                \n",
    "#encode validation set\n",
    "for j in X_test:\n",
    "    encoded_data_test = tokenizer.batch_encode_plus(j,\n",
    "                                                    add_special_tokens = True,\n",
    "                                                    return_attention_mask = True,\n",
    "                                                    pad_to_max_length = True,\n",
    "                                                    max_length = MAX_LENGTH,\n",
    "                                                    return_tensors = 'pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0a0ad904cb321023ffd122a6ae598c4dff0c10570398002785daccd0687b9d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
